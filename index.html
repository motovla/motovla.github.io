<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Generalist Robot Manipulation Beyond Action Labeled Data">
  <meta name="keywords" content="VLA, Generalist Robot Manipulation, Learning from Human Demonstrations">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generalist Robot Manipulation Beyond Action Labeled Data</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding-bottom: 1rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Generalist Robot Manipulation Beyond Action Labeled Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://aspiridon0v.github.io/">Alexander Spiridonov</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://jannicozaech.github.io/">Jan-Nico Zaech</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://insait.ai/nikolay-nikolov-2/">Nikolay Nikolov</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjg3LC0xOTcxNDY1MTc4.html">Luc Van Gool</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://insait.ai/dr-danda-paudel/">Danda Pani Paudel</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-bottom: 5px;">
            <span class="author-block"><sup>1</sup>INSAIT, Sofia University "St. Kliment Ohridski",</span>
            <span class="author-block"><sup>2</sup>ETH Zurich,</span>
          </div>

          <!-- Institution Logos -->
          <div class="has-text-centered" style="margin: 5px 0; padding: 0;">
            <div class="institution-logos" style="display: flex; justify-content: center; align-items: center; gap: 20px; flex-wrap: wrap;">
              <img src="./static/figures/insaitlogo.png" alt="INSAIT Logo" style="height: 30px; max-width: 250px; object-fit: cover; object-position: center;">
              <img src="./static/figures/ethlogo.png" alt="ETH Zurich Logo" style="height: 45px; max-width: 150px; object-fit: contain;">
            </div>
          </div>

          <div class="column has-text-centered" style="margin-top: 5px; padding-top: 0;">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Section -->
<section class="section" style="padding: 1rem 1.5rem 2.5rem 1.5rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div style="background-color: #f5f5f5; border-radius: 15px; padding: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
          <h2 class="title is-4">TL;DR</h2>
          <div class="content has-text-left">
            <ul style="font-size: 1.1rem; line-height: 1.6;">
              <li><strong>Problem:</strong> Robot manipulation methods require expensive action-labeled demonstration data.</li>
              <li><strong>Solution:</strong> Our model, MotoVLA, learns from unlabeled humand and robot videos using 3D point clouds, then aligns to robot actions with a smaller labeled dataset.</li>
              <li><strong>Results:</strong> Achieves strong performance in simulation and real-world experiments, and enables robots to learn new tasks without action labels.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/corl_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">MotoVLA</span> enables robot actions whose labels are not available during training by learning from unlabeled human and robot demonstrations.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Recent advances in generalist robot manipulation leverage pre-trained
          Vision Language Models (VLMs) and large-scale robot demonstrations to tackle
          diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization.</p>
          
          <p>To address this, we propose a method that benefits from videos without action labels—featuring humans and/or robots in action—enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment.</p>
          
          <p>We show that our method not only learns from unlabeled human and robot demonstrations—improving downstream generalist robot policies—but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/figures/Overview.jpg" alt="Method Overview" style="width: 100%; margin: 20px 0;">
          <p>
            Our method enables robot actions whose labels are not available during training. Such unlabeled demonstrations may come from humans or other robots performing them. This out-of-action domain generalization is achieved using large-scale dynamic point cloud-based training, followed by action alignment on a smaller dataset with action labels.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->

    <!-- Architecture. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Architecture</h2>
        <div class="content has-text-justified">
          <img src="./static/figures/Architecture.png" alt="Architecture" style="width: 100%; margin: 20px 0;">
          <p>
            We utilize a 3D Dynamics Predictor to learn an embodiment agnostic representation from unlabeled videos, followed by an Action Predictor for generalist manipulation. The approach uses a two-stage training process with dynamic point clouds as a common representation. We initialize the Action Predictor with the Dynamics Predictor and finetune it on a smaller dataset with action labels.
          </p>
        </div>
      </div>
    </div>
    <!--/ Architecture. -->

    <!-- In-Domain Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">In-Domain Results</h2>
        <div class="content has-text-justified">
          <img src="./static/figures/In-Domain.png" alt="In-Domain Results" style="width: 100%; margin: 20px 0;">
          <p>
            MotoVLA (R + H) achieves 68.2% average success rate in SIMPLER simulation, outperforming LAPA by 14.1% and π₀ baseline by 11.4%. Dynamic point cloud pre-training improves performance even for tasks with action supervision, demonstrating effective cross-domain motion prior learning.
          </p>
        </div>
      </div>
    </div>
    <!--/ In-Domain Results. -->

    <!-- Out-of-Domain Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Out-of-Domain Results</h2>
        <div class="content has-text-justified">
          <!-- Experiment Video -->
          <div style="margin: 20px 0;">
            <video id="experiment-video" autoplay muted loop playsinline style="width: 100%; max-width: 100%; height: auto;">
              <source src="./static/videos/ExpVideos.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          
          <img src="./static/figures/OOD-Results.png" alt="Out-of-Domain Results" style="width: 100%; margin: 20px 0;">
          <p>
            Our method achieves superior performance on real robot evaluation across 8 out-of-domain tasks. Significant improvements are observed for tasks present in human demonstration data (Push Button, Cube on Scale, Cable in Basket, Clamp in Cup), demonstrating direct knowledge transfer from unlabeled cross-embodiment demonstrations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Out-of-Domain Results. -->

  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{spiridonov2025MotoVLA,
  author    = {Spiridonov, Alexander and Zaech, Jan-Nico and Nikolov, Nikolay and Van Gool, Luc and Paudel, Danda Pani},
  title     = {Generalist Robot Manipulation Beyond Action Labeled Data},
  journal   = {CoRL},
  year      = {2025},
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="#" disabled>
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
            Template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
